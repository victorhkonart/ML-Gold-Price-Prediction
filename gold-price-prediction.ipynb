{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "import os\n",
    "\n",
    "for dirname, _, filenames in os.walk('kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv(\"kaggle/input/gold-price-prediction-dataset/FINAL_USO.csv\", na_values=['null'], index_col='Date', parse_dates=True, infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking missing values\n",
    "\n",
    "df_final.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# effect of index prices on gold rates\n",
    "\n",
    "GLD_adj_close = df_final['Adj Close']\n",
    "SPY_adj_close = df_final['SP_Ajclose']\n",
    "DJ_adj_close = df_final['DJ_Ajclose']\n",
    "\n",
    "df_p = pd.DataFrame({'GLD':GLD_adj_close, 'SPY':SPY_adj_close, 'DJ':DJ_adj_close})\n",
    "\n",
    "df_ax = df_p.plot(title='Effect of Index Prices on Gold Rates', figsize=(15,8))\n",
    "\n",
    "df_ax.set_ylabel('Price')\n",
    "df_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing daily returns of all features\n",
    "\n",
    "def compute_daily_returns(df):\n",
    "\n",
    "    \"\"\" compute and return the daily return values \"\"\"\n",
    "\n",
    "    # note: returned DataFrame must have the same number of rows\n",
    "    daily_return = (df/df.shift(1))-1\n",
    "    daily_return[0] = 0\n",
    "\n",
    "    return daily_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLD_adj_close = df_final['Adj Close']\n",
    "SPY_adj_close = df_final['SP_Ajclose']\n",
    "DJ_adj_close = df_final['DJ_Ajclose']\n",
    "EG_adj_close =  df_final['EG_Ajclose']\n",
    "USO_Adj_close = df_final['USO_Adj Close']\n",
    "GDX_Adj_close = df_final['GDX_Adj Close']\n",
    "EU_price = df_final['EU_Price']\n",
    "OF_price = df_final['OF_Price']\n",
    "OS_price = df_final['OS_Price']\n",
    "SF_price = df_final['SF_Price']\n",
    "USB_price = df_final['USB_Price']\n",
    "PLT_price = df_final['PLT_Price']\n",
    "PLD_price = df_final['PLD_Price']\n",
    "rho_price = df_final['RHO_PRICE']\n",
    "usdi_price = df_final['USDI_Price']\n",
    "\n",
    "GLD_daily_return = compute_daily_returns(GLD_adj_close)\n",
    "SPY_daily_return = compute_daily_returns(SPY_adj_close)\n",
    "DJ_adj_return = compute_daily_returns(DJ_adj_close)\n",
    "EG_adj_return = compute_daily_returns(EG_adj_close)\n",
    "USO_Adj_return = compute_daily_returns(USO_Adj_close)\n",
    "GDX_Adj_return = compute_daily_returns(GDX_Adj_close)\n",
    "EU_return = compute_daily_returns(EU_price)\n",
    "OF_price = compute_daily_returns(OF_price)\n",
    "OS_price = compute_daily_returns(OS_price)\n",
    "SF_price = compute_daily_returns(SF_price)\n",
    "USB_price = compute_daily_returns(USB_price)\n",
    "PLT_price = compute_daily_returns(PLT_price)\n",
    "PLD_price = compute_daily_returns(PLD_price)\n",
    "rho_price = compute_daily_returns(rho_price)\n",
    "USDI_price = compute_daily_returns(usdi_price)\n",
    "\n",
    "df_d = pd.DataFrame({'GLD':GLD_daily_return, 'SPY':SPY_daily_return, 'DJ':DJ_adj_return, 'EG':EG_adj_return, 'USO':USO_Adj_return, 'GDX':GDX_Adj_return,'EU':EU_return, 'OF':OF_price,'SF':SF_price,'OS':OS_price, 'USB':USB_price, 'PLT':PLT_price, 'PLD':PLD_price, 'RHO':rho_price,'USDI':USDI_price})\n",
    "\n",
    "daily_ax = df_d[-100:].plot(title='Last 100 records of daily return of all features', figsize=(15,8))\n",
    "\n",
    "daily_ax.set_ylabel('Daily return')\n",
    "daily_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing daily returns of stock indexes\n",
    "\n",
    "df_s = pd.DataFrame({'GLD':GLD_daily_return, 'SPY':SPY_daily_return, 'DJ':DJ_adj_return})\n",
    "\n",
    "daily_ax = df_s[-100:].plot(title='Last 100 records of daily return of Stock Indexes', figsize = (15,8))\n",
    "\n",
    "daily_ax.set_ylabel('Daily return')\n",
    "daily_ax.legend(loc = 'lower left')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot\n",
    "\n",
    "df_d.plot(kind = 'scatter', x = 'SPY', y = 'GLD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d.plot(kind = 'scatter', x = 'EG', y = 'GLD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d.plot(kind = 'scatter', x = 'USO', y = 'GLD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d.plot(kind = 'scatter', x = 'USB', y = 'GLD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d.plot(kind = 'scatter', x = 'EU', y = 'GLD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d.plot(kind = 'scatter', x = 'PLT', y = 'GLD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d.plot(kind = 'scatter', x = 'PLD', y = 'GLD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing mean,standard deviation and kurtosis of Gold ETF daily return\n",
    "\n",
    "mean = df_d['GLD'].mean()\n",
    "\n",
    "# computing standard deviation of Gold stock\n",
    "\n",
    "std = df_d['GLD'].std()\n",
    "kurt = df_d['GLD'].kurtosis()\n",
    "\n",
    "print('Mean = ', mean)\n",
    "print('Standard Deviation = ', std)\n",
    "print('Kurtosis = ', kurt)\n",
    "\n",
    "# plotting histogram\n",
    "\n",
    "df_d['GLD'].hist(bins = 20)\n",
    "\n",
    "plt.axvline(mean, color = 'w',linestyle = 'dashed',linewidth = 2)\n",
    "plt.axvline(std, color = 'r',linestyle = 'dashed',linewidth = 2)\n",
    "plt.axvline(-std, color = 'r',linestyle = 'dashed',linewidth = 2)\n",
    "\n",
    "plt.title(\"Plotting of Mean, Standard deviation and Kurtosis of Gold Prices\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# computing mean,standard deviation and kurtosis of S&P 500 Index daily return\n",
    "\n",
    "mean = df_d['SPY'].mean()\n",
    "\n",
    "# computing standard deviation of Gold stock\n",
    "\n",
    "std = df_d['SPY'].std()\n",
    "kurt = df_d['SPY'].kurtosis()\n",
    "\n",
    "print('Mean = ', mean)\n",
    "print('Standard Deviation = ', std)\n",
    "print('Kurtosis = ', kurt)\n",
    "\n",
    "# plotting histogram\n",
    "\n",
    "df_d['SPY'].hist(bins = 20)\n",
    "\n",
    "plt.axvline(mean, color = 'w',linestyle = 'dashed',linewidth = 2)\n",
    "plt.axvline(std, color = 'r',linestyle = 'dashed',linewidth = 2)\n",
    "plt.axvline(-std, color = 'r',linestyle = 'dashed',linewidth = 2)\n",
    "\n",
    "plt.title(\"Plotting of Mean, Standard deviation and Kurtosis of SPY Prices\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# computing mean,standard deviation and kurtosis of Dow Jones Index daily return\n",
    "\n",
    "mean = df_d['DJ'].mean()\n",
    "\n",
    "# computing standard deviation of Gold stock\n",
    "\n",
    "std = df_d['DJ'].std()\n",
    "kurt = df_d['DJ'].kurtosis()\n",
    "\n",
    "print('Mean = ', mean)\n",
    "print('Standard Deviation = ', std)\n",
    "print('Kurtosis = ', kurt)\n",
    "\n",
    "# plotting histogram\n",
    "\n",
    "df_d['DJ'].hist(bins = 20)\n",
    "\n",
    "plt.axvline(mean, color = 'w',linestyle = 'dashed',linewidth = 2)\n",
    "plt.axvline(std, color = 'r',linestyle = 'dashed',linewidth = 2)\n",
    "plt.axvline(-std, color = 'r',linestyle = 'dashed',linewidth = 2)\n",
    "\n",
    "plt.title(\"Plotting of Mean, Standard deviation and Kurtosis of Dow jones Prices\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation analysis \n",
    "\n",
    "# plotting correlation analysis\n",
    "\n",
    "plt.figure(figsize = (24,18)) \n",
    "\n",
    "sns.heatmap(df_final.corr(), annot = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_final.drop(['Adj Close'], axis = 1)\n",
    "X = X.drop(['Close'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X.corrwith(df_final['Adj Close']).plot.bar(figsize = (20, 10), title = \"Correlation with Adj Close\", fontsize = 20, rot = 90, grid = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df_final.corr()\n",
    "coef = corr_matrix[\"Adj Close\"].sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positively correlated variables\n",
    "\n",
    "pos_corr = coef[coef>0]\n",
    "pos_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negatively correlated variables\n",
    "\n",
    "neg_corr = coef[coef<0]\n",
    "neg_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# technical indicators\n",
    "\n",
    "def calculate_MACD(df, nslow = 26, nfast = 12):\n",
    "\n",
    "    emaslow = df.ewm(span = nslow, min_periods = nslow, adjust = True, ignore_na = False).mean()\n",
    "    emafast = df.ewm(span = nfast, min_periods = nfast, adjust = True, ignore_na = False).mean()\n",
    "    dif = emafast - emaslow\n",
    "    MACD = dif.ewm(span = 9, min_periods = 9, adjust = True, ignore_na = False).mean()\n",
    "\n",
    "    return dif, MACD\n",
    "\n",
    "def calculate_RSI(df, periods = 14):\n",
    "\n",
    "    # wilder's RSI\n",
    "    delta = df.diff()\n",
    "    up, down = delta.copy(), delta.copy()\n",
    "\n",
    "    up[up<0] = 0\n",
    "    down[down>0] = 0\n",
    "\n",
    "    rUp = up.ewm(com = periods, adjust = False).mean()\n",
    "    rDown = down.ewm(com = periods, adjust = False).mean().abs()\n",
    "\n",
    "    rsi = 100 - 100 / (1 + rUp / rDown)\n",
    "\n",
    "    return rsi\n",
    "\n",
    "def calculate_SMA(df, peroids = 15):\n",
    "\n",
    "    SMA = df.rolling(window = peroids, min_periods = peroids, center = False).mean()\n",
    "\n",
    "    return SMA\n",
    "\n",
    "def calculate_BB(df, peroids = 15):\n",
    "\n",
    "    STD = df.rolling(window = peroids, min_periods = peroids, center = False).std()\n",
    "    SMA = calculate_SMA(df)\n",
    "    upper_band = SMA + (2 * STD)\n",
    "    lower_band = SMA - (2 * STD)\n",
    "\n",
    "    return upper_band, lower_band\n",
    "\n",
    "def calculate_stdev(df,periods = 5):\n",
    "\n",
    "    STDEV = df.rolling(periods).std()\n",
    "    \n",
    "    return STDEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting technical indicators\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 4, figsize = (16, 4))\n",
    "\n",
    "# Calculate Simple Moving Average for GLD\n",
    "SMA_GLD = calculate_SMA(GLD_adj_close)\n",
    "\n",
    "GLD_adj_close[:365].plot(title = 'GLD Moving Average',label = 'GLD', ax = axes[0])\n",
    "\n",
    "SMA_GLD[:365].plot(label = \"SMA\",ax = axes[0])\n",
    "\n",
    "# Calculate Bollinger Bands for GLD\n",
    "upper_band, lower_band = calculate_BB(GLD_adj_close)\n",
    "\n",
    "upper_band[:365].plot(label = 'upper band', ax = axes[0])\n",
    "lower_band[:365].plot(label = 'lower band', ax = axes[0])\n",
    "\n",
    "\n",
    "# Calculate MACD for GLD\n",
    "DIF, MACD = calculate_MACD(GLD_adj_close)\n",
    "\n",
    "DIF[:365].plot(title = 'DIF and MACD',label = 'DIF', ax = axes[1])\n",
    "MACD[:365].plot(label = 'MACD', ax = axes[1])\n",
    "\n",
    "# Calculate RSI for GLD\n",
    "RSI = calculate_RSI(GLD_adj_close)\n",
    "RSI[:365].plot(title = 'RSI', label = 'RSI', ax = axes[2])\n",
    "\n",
    "# Calculating Standard deviation for GLD\n",
    "STDEV = calculate_stdev(GLD_adj_close)\n",
    "STDEV[:365].plot(title = 'STDEV', label = 'STDEV', ax = axes[3])\n",
    "\n",
    "Open_Close = df_final.Open - df_final.Close\n",
    "\n",
    "High_Low = df_final.High - df_final.Low\n",
    "\n",
    "axes[0].set_ylabel('Price')\n",
    "axes[1].set_ylabel('Price')\n",
    "axes[2].set_ylabel('Price')\n",
    "axes[3].set_ylabel('Price')\n",
    "\n",
    "axes[0].legend(loc = 'lower left')\n",
    "axes[1].legend(loc = 'lower left')\n",
    "axes[2].legend(loc = 'lower left')\n",
    "axes[3].legend(loc = 'lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (16,4))\n",
    "\n",
    "Open_Close = df_final.Open - df_final.Close\n",
    "Open_Close[:365].plot(title = 'Open-Close', label = 'Open_Close', ax = axes[0])\n",
    "\n",
    "High_Low = df_final.High - df_final.Low\n",
    "High_Low[:365].plot(title = 'High_Low', label = 'High_Low', ax = axes[1])\n",
    "\n",
    "axes[0].set_ylabel('Price')\n",
    "axes[1].set_ylabel('Price')\n",
    "\n",
    "axes[0].legend(loc = 'lower left')\n",
    "axes[1].legend(loc = 'lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_final\n",
    "\n",
    "test['SMA'] = SMA_GLD\n",
    "test['Upper_band'] = upper_band\n",
    "test['Lower_band'] = lower_band\n",
    "test['DIF'] = DIF\n",
    "test['MACD'] = MACD\n",
    "test['RSI'] = RSI\n",
    "test['STDEV'] = STDEV\n",
    "test['Open_Close'] = Open_Close\n",
    "test['High_Low'] = High_Low\n",
    "\n",
    "# dropping first 33 records from the data as it has null values because of introduction of technical indicators\n",
    "test = test[33:]\n",
    "\n",
    "# target column\n",
    "target_adj_close = pd.DataFrame(test['Adj Close'])\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this step I would segregate feature and target variables\n",
    "# I will not use Close feature of GLD ETF and will use Adjusted Close of Gold ETF as target variable\n",
    "\n",
    "# selecting feature columns\n",
    "feature_columns = ['Open','High','Low','Volume','SP_open','SP_high','SP_low','SP_Ajclose','SP_volume','DJ_open','DJ_high','DJ_low','DJ_Ajclose','DJ_volume','EG_open','EG_high','EG_low','EG_Ajclose','EG_volume','EU_Price','EU_open','EU_high','EU_low','EU_Trend','OF_Price','OF_Open','OF_High','OF_Low','OF_Volume','OF_Trend','OS_Price','OS_Open','OS_High','OS_Low','OS_Trend','SF_Price','SF_Open','SF_High','SF_Low','SF_Volume','SF_Trend','USB_Price','USB_Open','USB_High','USB_Low','USB_Trend','PLT_Price','PLT_Open','PLT_High','PLT_Low','PLT_Trend','PLD_Price','PLD_Open','PLD_High','PLD_Low','PLD_Trend','RHO_PRICE','USDI_Price','USDI_Open','USDI_High','USDI_Low','USDI_Volume','USDI_Trend','GDX_Open','GDX_High','GDX_Low','GDX_Close','GDX_Adj Close','GDX_Volume','USO_Open','USO_High','USO_Low','USO_Close','USO_Adj Close','USO_Volume','SMA','Upper_band','Lower_band','DIF','MACD','RSI','STDEV','Open_Close','High_Low']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the data\n",
    "\n",
    "# perform feature scaling/normalization of feature variables using sklearn's MinMaxScaler function\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "feature_minmax_transform_data = scaler.fit_transform(test[feature_columns])\n",
    "feature_minmax_transform = pd.DataFrame(columns = feature_columns, data = feature_minmax_transform_data, index = test.index)\n",
    "\n",
    "feature_minmax_transform.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of features: ', feature_minmax_transform.shape)\n",
    "print('Shape of target: ', target_adj_close.shape)\n",
    "\n",
    "# shift target array because we want to predict the n+1 day value\n",
    "target_adj_close = target_adj_close.shift(-1)\n",
    "validation_y = target_adj_close[-90:-1]\n",
    "target_adj_close = target_adj_close[:-90]\n",
    "\n",
    "# taking last 90 rows of data to be validation set\n",
    "validation_X = feature_minmax_transform[-90:-1]\n",
    "feature_minmax_transform = feature_minmax_transform[:-90]\n",
    "\n",
    "display(validation_X.tail())\n",
    "display(validation_y.tail())\n",
    "\n",
    "print(\"\\n ---- After process ----\\n\")\n",
    "print('Shape of features: ', feature_minmax_transform.shape)\n",
    "print('Shape of target: ', target_adj_close.shape)\n",
    "\n",
    "display(target_adj_close.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "\n",
    "# perform train test split using sklearn's timeseries split\n",
    "ts_split = TimeSeriesSplit(n_splits = 10)\n",
    "\n",
    "for train_index, test_index in ts_split.split(feature_minmax_transform):\n",
    "          X_train, X_test = feature_minmax_transform[:len(train_index)], feature_minmax_transform[len(train_index): (len(train_index)+len(test_index))]\n",
    "\n",
    "          y_train, y_test = target_adj_close[:len(train_index)].values.ravel(), target_adj_close[len(train_index): (len(train_index)+len(test_index))].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_result(model, model_name):\n",
    "\n",
    "    predicted = model.predict(validation_X)\n",
    "    RMSE_score = np.sqrt(mean_squared_error(validation_y, predicted))\n",
    "\n",
    "    print('RMSE: ', RMSE_score)\n",
    "\n",
    "    R2_score = r2_score(validation_y, predicted)\n",
    "\n",
    "    print('R2 score: ', R2_score)\n",
    "\n",
    "    plt.plot(validation_y.index, predicted, 'r', label = 'Predict')\n",
    "    plt.plot(validation_y.index, validation_y, 'b', label = 'Actual')\n",
    "\n",
    "    plt.ylabel('Price')\n",
    "\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator())\n",
    "\n",
    "    plt.title(model_name + ' Predict vs Actual')\n",
    "    plt.legend(loc = 'upper right')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model building\n",
    "\n",
    "# benchmark model: will use decision tree regressor with default parameter as benchmark model for the project\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor(random_state = 0)\n",
    "\n",
    "benchmark_dt = dt.fit(X_train, y_train)\n",
    "validate_result(benchmark_dt, 'Decision Tree Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution model\n",
    "\n",
    "# support vector regressor (SVR)\n",
    "\n",
    "# save all solution models\n",
    "solution_models = {}\n",
    "\n",
    "# SVR with linear Kernel\n",
    "svr_lin = SVR(kernel = 'linear')\n",
    "linear_svr_clf_feat = svr_lin.fit(X_train, y_train)\n",
    "validate_result(linear_svr_clf_feat, 'Linear SVR All Feat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will tune two parameters of SVR, C and epsilon, to see if the model show any improvement\n",
    "linear_svr_parameters = {    \n",
    "    'C':[0.5, 1.0, 10.0, 50.0],\n",
    "    'epsilon':[0, 0.1, 0.5, 0.7, 0.9],\n",
    "}\n",
    "\n",
    "lsvr_grid_search_feat = GridSearchCV(estimator = linear_svr_clf_feat, param_grid = linear_svr_parameters, cv = ts_split)\n",
    "\n",
    "lsvr_grid_search_feat.fit(X_train, y_train)\n",
    "\n",
    "validate_result(lsvr_grid_search_feat, 'Linear SVR GS All Feat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using gridsearch on SVR, we get significant improvement in R2 score and RMSE\n",
    "# save this as our first solution model\n",
    "\n",
    "solution_models['SVR All Feat'] = lsvr_grid_search_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution model\n",
    "\n",
    "# random forest\n",
    "rf_cl = RandomForestRegressor(n_estimators = 50, random_state = 0)\n",
    "\n",
    "random_forest_clf_feat = rf_cl.fit(X_train, y_train)\n",
    "\n",
    "validate_result(random_forest_clf_feat, 'Random Forest with ALL Feat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning\n",
    "\n",
    "# will tune 3 parameters of random forest (n_estimators, max_features, max_depth)\n",
    "random_forest_parameters = {\n",
    "    'n_estimators':[10,15,20, 50, 100],\n",
    "    'max_features':['auto','sqrt','log2'],\n",
    "    'max_depth':[2, 3, 5, 7,10],\n",
    "}\n",
    "\n",
    "grid_search_RF_feat = GridSearchCV(estimator = random_forest_clf_feat, param_grid = random_forest_parameters, cv = ts_split)\n",
    "\n",
    "grid_search_RF_feat.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search_RF_feat.best_params_)\n",
    "validate_result(grid_search_RF_feat, 'RandomForest GS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest with default parameters performed better than tuned random forest model\n",
    "# will include random forest with default parameters as our second solution model\n",
    "\n",
    "solution_models['Random_Forest with Feat'] = random_forest_clf_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution model\n",
    "\n",
    "# lasso and ridge\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "lasso_clf = LassoCV(n_alphas = 1000, max_iter = 3000, random_state = 0)\n",
    "ridge_clf = RidgeCV(gcv_mode = 'auto')\n",
    "\n",
    "lasso_clf_feat = lasso_clf.fit(X_train, y_train)\n",
    "validate_result(lasso_clf_feat, 'LassoCV')\n",
    "solution_models['LassoCV All Feat'] = lasso_clf_feat\n",
    "\n",
    "ridge_clf_feat = ridge_clf.fit(X_train, y_train)\n",
    "validate_result(ridge_clf_feat, 'RidgeCV')\n",
    "solution_models['RidgeCV All Feat'] = ridge_clf_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution model\n",
    "\n",
    "# bayesian ridge\n",
    "from sklearn import linear_model\n",
    "\n",
    "bay = linear_model.BayesianRidge()\n",
    "bay_feat = bay.fit(X_train, y_train)\n",
    "validate_result(bay_feat, 'Bayesian')\n",
    "solution_models['Bay All Feat'] = bay_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution model\n",
    "\n",
    "# gradient boosting regressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "regr = GradientBoostingRegressor(n_estimators = 70, learning_rate = 0.1, max_depth = 4, random_state = 0, loss = 'squared_error')\n",
    "GB_feat = regr.fit(X_train, y_train)\n",
    "validate_result(GB_feat, 'NB')\n",
    "solution_models['GB All Feat'] = GB_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution model\n",
    "\n",
    "# stochastic gradient descent (SGD)\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd = SGDRegressor(max_iter = 1000, tol = 1e-3, loss = 'squared_epsilon_insensitive', penalty = 'l1', alpha = 0.1)\n",
    "sgd_feat = sgd.fit(X_train, y_train)\n",
    "validate_result(sgd_feat, 'SGD')\n",
    "solution_models['SGD All Feat'] = sgd_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model review\n",
    "\n",
    "# will review benchmark model and all solution models, based on evaluation metrics i.e, RMSE and R2 score\n",
    "RMSE_scores = {}\n",
    "\n",
    "def model_review(models):\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(16, 16))\n",
    "\n",
    "    #plot benchmark model\n",
    "    benchmark_predicted = benchmark_dt.predict(validation_X)\n",
    "    benchmark_RSME_score = np.sqrt(mean_squared_error(validation_y, benchmark_predicted))\n",
    "    RMSE_scores['Benchmark'] = benchmark_RSME_score\n",
    "    \n",
    "    axes[0,0].plot(validation_y.index, benchmark_predicted,'r', label='Predict')\n",
    "    axes[0,0].plot(validation_y.index, validation_y,'b', label='Actual')\n",
    "\n",
    "    axes[0,0].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    axes[0,0].xaxis.set_major_locator(mdates.MonthLocator())\n",
    "\n",
    "    axes[0,0].set_ylabel('Price')\n",
    "    axes[0,0].set_title(\"Benchmark Predict's RMSE Error: \" +\"{0:.2f}\".format(benchmark_RSME_score))\n",
    "    axes[0,0].legend(loc='upper right')\n",
    "    \n",
    "    #plot block\n",
    "    ax_x = 0\n",
    "    ax_y = 1\n",
    "\n",
    "    #plot solution model\n",
    "    for name, model in models.items():\n",
    "\n",
    "        predicted = model.predict(validation_X)\n",
    "        RSME_score = np.sqrt(mean_squared_error(validation_y, predicted))\n",
    "\n",
    "        axes[ax_x][ax_y].plot(validation_y.index, predicted,'r', label='Predict')\n",
    "        axes[ax_x][ax_y].plot(validation_y.index, validation_y,'b', label='Actual')\n",
    "\n",
    "        axes[ax_x][ax_y].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "        axes[ax_x][ax_y].xaxis.set_major_locator(mdates.MonthLocator())\n",
    "\n",
    "        axes[ax_x][ax_y].set_ylabel('Price')\n",
    "        axes[ax_x][ax_y].set_title(name + \"'s RMSE Error: \" +\"{0:.2f}\".format(RSME_score))\n",
    "        axes[ax_x][ax_y].legend(loc='upper right')\n",
    "\n",
    "        RMSE_scores[name] = RSME_score\n",
    "\n",
    "        if ax_x <=2:\n",
    "            if ax_y < 2:\n",
    "                ax_y += 1\n",
    "            else:\n",
    "                ax_x += 1\n",
    "                ax_y = 0\n",
    "                \n",
    "    plt.show()\n",
    "\n",
    "model_review(solution_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison of RMSE of benchmark and all solution models\n",
    "\n",
    "model_names = []\n",
    "model_values = []\n",
    "\n",
    "for name, value in RMSE_scores.items():\n",
    "    model_names.append(name)\n",
    "    model_values.append(value)\n",
    "\n",
    "model_values = np.array(model_values)\n",
    "model_names = np.array(model_names)\n",
    "\n",
    "indices = np.argsort(model_values)\n",
    "columns = model_names[indices[:8]]\n",
    "values = model_values[indices][:8]\n",
    "\n",
    "fig = plt.figure(figsize = (16,8))\n",
    "\n",
    "plt.bar(np.arange(8), values, width = 0.6, align = \"center\", color = '#ff00c1')\n",
    "plt.xticks(np.arange(8), columns)\n",
    "\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE compare')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "\n",
    "# will select supporting features using sklearn's SelectFromModel library, using Lasso regressor as it has lowest RMSE\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "sfm = SelectFromModel(lasso_clf_feat)\n",
    "sfm.fit(feature_minmax_transform, target_adj_close.values.ravel())\n",
    "\n",
    "display(feature_minmax_transform.head())\n",
    "\n",
    "sup = sfm.get_support()\n",
    "zipped = zip(feature_minmax_transform, sup)\n",
    "\n",
    "print(*zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting features which supports model building process\n",
    "feature_selected = feature_minmax_transform[['Open','High','Low','OF_Trend','USB_Trend','PLT_Trend','USDI_Price','GDX_Close','SMA','Upper_band','RSI','Open_Close']]\n",
    "\n",
    "feature_selected_validation_X = validation_X[['Open','High','Low','OF_Trend','USB_Trend','PLT_Trend','USDI_Price','GDX_Close','SMA','Upper_band','RSI','Open_Close']]\n",
    "\n",
    "display(feature_selected.head())\n",
    "display(feature_selected_validation_X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "for train_index, test_index in ts_split.split(feature_selected):\n",
    "    X_train, X_test = feature_selected[:len(train_index)], feature_selected[len(train_index): (len(train_index)+len(test_index))]\n",
    "\n",
    "    y_train, y_test = target_adj_close[:len(train_index)].values.ravel(), target_adj_close[len(train_index): (len(train_index)+len(test_index))].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation feature selected benchmark & solution model\n",
    "def feature_selected_validate_result(model, model_name):\n",
    "    predicted = model.predict(feature_selected_validation_X)\n",
    "    \n",
    "    RSME_score = np.sqrt(mean_squared_error(validation_y, predicted))\n",
    "    R2_score = r2_score(validation_y, predicted)\n",
    "    \n",
    "    print(model_name + '\\n')\n",
    "    print('RMSE: ', RSME_score)\n",
    "    print('R2 score: ', R2_score)\n",
    "    print('----------------------')\n",
    "\n",
    "\n",
    "print('---------Benchmark-------------') \n",
    "\n",
    "dt = DecisionTreeRegressor(random_state = 0)\n",
    "benchmark_dt_fs = dt.fit(X_train, y_train)\n",
    "\n",
    "feature_selected_validate_result(benchmark_dt_fs, 'Decision Tree')\n",
    "\n",
    "feature_selected_solution_models = {}\n",
    " \n",
    "print('--------Solution Models--------------')  \n",
    "\n",
    "# random forest\n",
    "random_forest_clf_fs = RandomForestRegressor(random_state = 0, max_depth = 3, max_features = 'auto', n_estimators = 10)\n",
    "\n",
    "random_forest_parameters = {\n",
    "    'n_estimators':[10, 50, 100],\n",
    "    'max_features':['auto','sqrt','log2'],\n",
    "    'max_depth':[3, 5, 7],\n",
    "}\n",
    "\n",
    "grid_search_RF_fs = GridSearchCV(estimator = random_forest_clf_fs, param_grid = random_forest_parameters, cv = ts_split,)\n",
    "grid_search_RF_fs.fit(X_train, y_train)\n",
    "\n",
    "feature_selected_validate_result(grid_search_RF_fs,'Feature selected RandomForest GS')\n",
    "feature_selected_solution_models['FS_RandomForest'] = grid_search_RF_fs\n",
    "\n",
    "# Linear SVR\n",
    "linear_svr_fs = SVR(C = 50.0, epsilon = 0, kernel = 'linear')\n",
    "linear_svr_clf_fs = linear_svr_fs.fit(X_train,y_train)\n",
    "\n",
    "feature_selected_validate_result(linear_svr_clf_fs,'Feature selected LSVR')\n",
    "feature_selected_solution_models['FS_LSVR'] = linear_svr_clf_fs\n",
    "\n",
    "\n",
    "# lasso\n",
    "lasso_fs = LassoCV(n_alphas = 1000, max_iter = 3000, random_state = 0)\n",
    "lasso_clf_fs = lasso_fs.fit(X_train,y_train)\n",
    "\n",
    "feature_selected_validate_result(lasso_clf_fs,'Feature selected LassoCV')\n",
    "feature_selected_solution_models['FS_Lasso'] = lasso_clf_fs\n",
    "\n",
    "# ridge\n",
    "ridge_fs = RidgeCV(gcv_mode = 'auto')\n",
    "ridge_clf_fs = ridge_fs.fit(X_train,y_train)\n",
    "\n",
    "feature_selected_validate_result(ridge_clf_fs,'Feature selected RidgeCV')\n",
    "feature_selected_solution_models['FS_RidgeCV'] = ridge_clf_fs\n",
    "\n",
    "# bayesian ridge\n",
    "bay = linear_model.BayesianRidge()\n",
    "bay_feat_fs = bay.fit(X_train,y_train)\n",
    "\n",
    "feature_selected_validate_result(bay_feat_fs,'Feature selected BayRidge')\n",
    "feature_selected_solution_models['Bay_Ridge'] = bay_feat_fs\n",
    "\n",
    "# gradient boosting\n",
    "regr =GradientBoostingRegressor(n_estimators = 70, learning_rate = 0.1,max_depth = 4, random_state = 0, loss = 'ls')\n",
    "GB_fs = regr.fit(X_train,y_train)\n",
    "\n",
    "feature_selected_validate_result(GB_fs,'Feature selected GB')\n",
    "feature_selected_solution_models['GB_FS'] = GB_fs\n",
    "\n",
    "# SGD\n",
    "sgd = SGDRegressor(max_iter = 1000, tol = 1e-3, loss = 'squared_epsilon_insensitive', penalty = 'l1', alpha = 0.1)\n",
    "sgd_fs = sgd.fit(X_train,y_train)\n",
    "\n",
    "feature_selected_validate_result(sgd_fs,'Feature selected SGD')\n",
    "feature_selected_solution_models['sgd_fs'] = sgd_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model review\n",
    "FS_RMSE_scores = {}\n",
    "\n",
    "def fs_model_review(models):\n",
    "\n",
    "    fig, axes = plt.subplots(nrows = 3, ncols = 3, figsize = (16,16))\n",
    "\n",
    "    # plot benchmark model\n",
    "    benchmark_dt_predicted = benchmark_dt_fs.predict(feature_selected_validation_X)\n",
    "    benchmark_RMSE_score = np.sqrt(mean_squared_error(validation_y, benchmark_dt_predicted))\n",
    "    FS_RMSE_scores['Benchmark'] = benchmark_RMSE_score\n",
    "\n",
    "    axes[0,0].plot(validation_y.index, benchmark_dt_predicted, 'y', label = 'Predict')\n",
    "    axes[0,0].plot(validation_y.index, validation_y, 'b', label = 'Actual')\n",
    "\n",
    "    axes[0,0].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    axes[0,0].xaxis.set_major_locator(mdates.MonthLocator())\n",
    "\n",
    "    axes[0,0].set_ylabel('Price')\n",
    "    axes[0,0].set_title(\"Benchmark Predict's RMSE Error: \" + \"{0:.2f}\".format(benchmark_RMSE_score))\n",
    "\n",
    "    axes[0,0].legend(loc = 'upper right')\n",
    "\n",
    "    # plot block\n",
    "    ax_x = 0\n",
    "    ax_y = 1\n",
    "\n",
    "    # plot solution model\n",
    "    for name, model in models.items():\n",
    "\n",
    "        predicted = model.predict(feature_selected_validation_X)\n",
    "\n",
    "        RMSE_score = np.sqrt(mean_squared_error(validation_y, predicted))\n",
    "        R2_score = r2_score(validation_y, predicted)\n",
    "\n",
    "        axes[ax_x][ax_y].plot(validation_y.index, predicted, 'y', label = 'Predict')\n",
    "        axes[ax_x][ax_y].plot(validation_y.index, validation_y, 'b', label = 'Actual')\n",
    "\n",
    "        axes[ax_x][ax_y].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "        axes[ax_x][ax_y].xaxis.set_major_locator(mdates.MonthLocator())\n",
    "\n",
    "        axes[ax_x][ax_y].set_ylabel('Price')\n",
    "        axes[ax_x][ax_y].set_title(name + \"'s RMSE Error: \" + \"{0:.2f}\".format(RMSE_score))\n",
    "\n",
    "        axes[ax_x][ax_y].legend(loc = 'upper right')\n",
    "\n",
    "        FS_RMSE_scores[name] = RMSE_score\n",
    "\n",
    "        if ax_x<=2:\n",
    "            if ax_y<2:\n",
    "                ax_y += 1\n",
    "            else:\n",
    "                ax_x += 1\n",
    "                ax_y = 0\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "fs_model_review(feature_selected_solution_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison of RMSE of feature selected models and original features model\n",
    "fs_model_names = []\n",
    "fs_model_values = []\n",
    "\n",
    "for name, value in FS_RMSE_scores.items():\n",
    "\n",
    "    fs_model_names.append(name)\n",
    "    fs_model_values.append(value)\n",
    "\n",
    "fs_model_values = np.array(fs_model_values)\n",
    "fs_model_names = np.array(fs_model_names)\n",
    "\n",
    "fs_indices = np.argsort(fs_model_values)\n",
    "\n",
    "fs_columns = fs_model_names[fs_indices[:8]]\n",
    "fs_values = fs_model_values[fs_indices][:8]\n",
    "\n",
    "origin_values = model_values[fs_indices][:8]\n",
    "\n",
    "fig = plt.figure(figsize = (16,8))\n",
    "\n",
    "plt.bar(np.arange(8) - 0.2, origin_values, width = 0.4, align = \"center\", color = \"#b2b2ff\", label = \"Original\")\n",
    "plt.bar(np.arange(8), fs_values, width = 0.4, align = \"center\", color = \"#3232ff\", label = \"Feature Selected\")\n",
    "\n",
    "plt.xticks(np.arange(8), fs_columns)\n",
    "\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('RMSE compare after feature selection')\n",
    "\n",
    "plt.legend(loc = 'upper center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT\n",
    "\n",
    "# will ensemble top three performing models i.e, in case of all the features model Lasso, Bayesian Ridge and Ridge are the best performing models;\n",
    "# while in case of feature selected models, will combine Lasso, Bayesian Ridge and Ridge, will compare all the future ensemble models with feature selected ensemble models\n",
    "\n",
    "# choosing the top three performing models to ensemble them\n",
    "ensemble_solution_models = [lasso_clf_feat, bay_feat, ridge_clf_feat]\n",
    "\n",
    "class EnsembleSolution:\n",
    "\n",
    "    models = []\n",
    "\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        for i in self.models:\n",
    "            i.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        result = 0\n",
    "        \n",
    "        for i in self.models:\n",
    "            result = result + i.predict(X)\n",
    "\n",
    "        result = result / len(self.models)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ensemble Solution Model with Original Features\")\n",
    "\n",
    "EnsembleModel = EnsembleSolution(ensemble_solution_models)\n",
    "validate_result(EnsembleModel, 'EnsembleSolution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_solution_model_fs = [lasso_clf_fs, bay_feat_fs, linear_svr_clf_fs]\n",
    "\n",
    "print(\"Ensemble Solution Model with Selected Features\")\n",
    "\n",
    "EnsembleModel_fs = EnsembleSolution(ensemble_solution_model_fs)\n",
    "feature_selected_validate_result(EnsembleModel_fs, 'EnsembleSolution with FS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model multiple times\n",
    "\n",
    "# by the train_reg_multipletimes function would train the model several times (in this case, 7 times), and use different parameters on TimeSeriesSplit in each time, average the R2 and RMSE\n",
    "# will apply this function on benchmark model and on top perfoming solution models with all features (which are Linear SVR, Lasso, Ridge and Bayesian Ridge), and compare the same\n",
    "\n",
    "def train_reg_multipletimes(model, times):\n",
    "\n",
    "    total_rmse = 0\n",
    "    total_r2 = 0\n",
    "\n",
    "    for i in range(times):\n",
    "\n",
    "        reg = model\n",
    "\n",
    "        for train_index, test_index in TimeSeriesSplit(n_splits=i+2).split(feature_minmax_transform):\n",
    "\n",
    "            X_train, X_test = feature_minmax_transform[:len(train_index)], feature_minmax_transform[len(train_index): (len(train_index)+len(test_index))]\n",
    "            y_train, y_test = target_adj_close[:len(train_index)].values.ravel(), target_adj_close[len(train_index): (len(train_index)+len(test_index))].values.ravel()\n",
    "            reg.fit(X_train, y_train)\n",
    "\n",
    "        predicted = reg.predict(validation_X)\n",
    "        rmse, r2 = print_result(validation_y, predicted, [0,len(validation_y)])\n",
    "\n",
    "        total_rmse += rmse\n",
    "        total_r2 += r2\n",
    "\n",
    "    return total_rmse / times, total_r2 / times\n",
    "\n",
    "def print_result(actual, predict, index):\n",
    "\n",
    "    RMSE_score = np.sqrt(mean_squared_error(actual, predict))\n",
    "\n",
    "    print('From {} to {}'.format(index[0],index[-1]))\n",
    "    print('RMSE: ', RMSE_score)\n",
    "\n",
    "    R2_score = r2_score(actual, predict)\n",
    "\n",
    "    print('R2 score: ', R2_score)\n",
    "    print('---------------------')\n",
    "\n",
    "    return RMSE_score, R2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Benchmark')\n",
    "\n",
    "t_multiple_benchmark_RMSE, t_multiple_benchmark_R2 = train_reg_multipletimes(benchmark_dt, 7)\n",
    "print('RMSE: {} // R2: {}'.format(t_multiple_benchmark_RMSE, t_multiple_benchmark_R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LSVR')\n",
    "\n",
    "t_multiple_LSVR_RMSE, t_multiple_LSVR_R2 = train_reg_multipletimes(linear_svr_clf_feat, 7)\n",
    "print('RMSE: {} // R2: {}'.format(t_multiple_LSVR_RMSE, t_multiple_LSVR_R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lasso')\n",
    "\n",
    "t_multiple_lasso_RMSE, t_multiple_lasso_R2 = train_reg_multipletimes(lasso_clf_feat, 7)\n",
    "print('RMSE: {} // R2: {}'.format(t_multiple_lasso_RMSE, t_multiple_lasso_R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ridge')\n",
    "\n",
    "t_multiple_ridge_RMSE, t_multiple_ridge_R2 = train_reg_multipletimes(ridge_clf_feat, 7)\n",
    "print('RMSE: {} // R2: {}'.format(t_multiple_ridge_RMSE, t_multiple_ridge_R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('BayRidge')\n",
    "\n",
    "t_multiple_bayridge_RMSE, t_multiple_bayridge_R2 = train_reg_multipletimes(bay_feat, 7)\n",
    "print('RMSE: {} // R2: {}'.format(t_multiple_bayridge_RMSE, t_multiple_bayridge_R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ensemble')\n",
    "\n",
    "t_multiple_ensemble_RMSE, t_multiple_ensemble_R2 = train_reg_multipletimes(EnsembleSolution(ensemble_solution_models), 7)\n",
    "print('RMSE: {} // R2: {}'.format(t_multiple_ensemble_RMSE, t_multiple_ensemble_R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(model, ts_split):\n",
    "\n",
    "    clf = model\n",
    "\n",
    "    total_rmse = 0\n",
    "    total_r2 = 0\n",
    "    \n",
    "    count = 0\n",
    "\n",
    "    for train_index, test_index in ts_split.split(validation_X):\n",
    "\n",
    "        X_test1, X_test2 = validation_X[:len(train_index)], validation_X[len(train_index): (len(train_index)+len(test_index))]\n",
    "        y_test1, y_test2 = validation_y[:len(train_index)].values.ravel(), validation_y[len(train_index): (len(train_index)+len(test_index))].values.ravel()\n",
    "\n",
    "        predicted_test1 = clf.predict(X_test1)\n",
    "        temp1_RMSE, temp1_R2 = print_result(y_test1, predicted_test1, train_index)\n",
    "\n",
    "        predicted_test2 = clf.predict(X_test2)\n",
    "        temp2_RMSE, temp2_R2 = print_result(y_test2, predicted_test2, test_index)\n",
    "\n",
    "        total_rmse += temp1_RMSE + temp2_RMSE\n",
    "        total_r2 += temp1_R2 + temp2_R2\n",
    "\n",
    "        count += 2\n",
    "\n",
    "    return total_rmse / count, total_r2 / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation\n",
    "\n",
    "timeseries_cv = TimeSeriesSplit(n_splits = 10)\n",
    "test_bench_RMSE, test_bench_R2 = cross_validate(benchmark_dt, timeseries_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lsvr_RMSE, test_lsvr_R2 = cross_validate(lsvr_grid_search_feat, timeseries_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ridge_RMSE, test_ridge_R2 = cross_validate(ridge_clf_feat, timeseries_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lasso_RMSE, test_lasso_R2 = cross_validate(lasso_clf_feat, timeseries_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bay_RMSE, test_bay_R2 = cross_validate(bay_feat, timeseries_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ensemble_RMSE, test_ensemble_R2 = cross_validate(EnsembleSolution(ensemble_solution_models), timeseries_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Benchmark RMSE: {} // Benchmark R2: {}'.format(test_bench_RMSE, test_bench_R2))\n",
    "\n",
    "print('LSVR RMSE: {} // LSVR R2: {}'.format(test_lsvr_RMSE, test_lsvr_R2))\n",
    "\n",
    "print('Lasso RMSE: {} // Lasso R2: {}'.format(test_lasso_RMSE, test_lasso_R2))\n",
    "\n",
    "print('Bayesian Ridge RMSE: {} // Bayesian Ridge R2: {}'.format(test_bay_RMSE, test_bay_R2))\n",
    "\n",
    "print('Ensemble RMSE: {} // Ensemble R2 {}'.format(test_ensemble_RMSE, test_ensemble_R2))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ec86887c1b68a826eb7e8a9863dab2e05fc23100eabd489883cb94169ae53c42"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
